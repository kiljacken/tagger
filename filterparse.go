package tagger

//go:generate go tool yacc -o filterparse_gen.go filterparse.y

import (
	"errors"
	"fmt"
	"io"
	"io/ioutil"
	"log"
	"regexp"
	"strconv"
)

func makeErr(part, msg string) error {
	return fmt.Errorf("tagger: Error while parsing %s:\n\t%s", part, msg)
}

// ParseFilter parses a filter from a reader, and returns the described
// filter.
//
// The format of the filter is a simple logic language.
// There is a simple precedence heirachy:
// 1. Parentheses
// 2. And expressions
// 3. Or expressions
// 4. Tags and comparators
//
// This means that "tag1 && tag2 && tag3 || tag4" parses as equivalent to
// "(tag1 && tag2 && tag3) || tag4".
//
// Examples of filters:
// "picture && year > 2007 && year < 2009"
// "todo && (important || easy)"
func ParseFilter(reader io.Reader) (Filter, error) {
	// Lex the input
	tokens, err := lexer(reader)
	if err != nil {
		return nil, err
	}

	// Call the generated parser
	ret := yyParse(tokens)
	if ret != 0 {
		// TODO: Make a proper error message
		return nil, errors.New("tagger: something bad happened yo")
	}

	// Return the generated filter
	return tokens.filter, nil
}

// tokenType is a wrapper around the tokens generated by yacc
type tokenType int

const (
	tokLparen tokenType = LPAREN
	tokRparen           = RPAREN
	tokAnd              = AND
	tokOr               = OR
	tokComp             = COMP
	tokTag              = TAG
	tokVal              = VAL
)

func (t tokenType) String() string {
	switch t {
	case tokLparen:
		return "LPAREN"
	case tokRparen:
		return "RPAREN"
	case tokAnd:
		return "AND"
	case tokOr:
		return "OR"
	case tokComp:
		return "COMP"
	case tokTag:
		return "TAG"
	case tokVal:
		return "VAL"
	default:
		return "INVALID"
	}
}

// token describes an indivdual token
type token struct {
	typ   tokenType
	value string
}

// tokenDef defines which pattern matches a given token type
type tokenDef struct {
	pattern string
	typ     tokenType
}

var tokenDefs = []tokenDef{
	{`[ \"\'\n\r]+`, -1},
	{`\(`, tokLparen},
	{`\)`, tokRparen},
	{`&&`, tokAnd},
	{`\|\|`, tokOr},
	{`==|!=|>=|<=|>|<`, tokComp},
	{`[a-zA-Z][a-zA-Z0-9_\-\?\(\)]*`, tokTag},
	{`-?[0-9]+`, tokVal},
}

func lexer(reader io.Reader) (*lex, error) {
	// Read the whole input stream into a string
	inputBytes, err := ioutil.ReadAll(reader)
	if err != nil {
		return nil, err
	}

	// Convert input to string
	input := string(inputBytes)

	// Prepare an empty array of tokens
	var tokens []token

outer:
	// Loop until we reach the end of our input
	for pos := 0; pos < len(input); {
		// Loop through all token definitions
		for _, tokenDef := range tokenDefs {
			// Compile the pattern
			// TODO: Precompile for increased performance
			patt := regexp.MustCompile(tokenDef.pattern)

			// If the pattern matches, and starts at the first character (0-indexed)
			if matches := patt.FindStringIndex(input[pos:]); matches != nil && matches[0] == 0 {
				// Capture the start and end position
				start, end := matches[0], matches[1]

				// Extract the value from our input
				value := input[pos+start : pos+end]

				// Increment our input position
				pos += len(value)

				// If the token is not whitespace, add it to the array of tokens
				if tokenDef.typ != -1 {
					token := token{value: value, typ: tokenDef.typ}
					tokens = append(tokens, token)
				}

				// Continue with next iteration of the outer loop, skipping the rest of the code
				continue outer
			}
		}

		// This point should only be reached if we reach something not matching
		// any token patterns, so we return an error.
		return nil, fmt.Errorf("tagger: Invalid char in filter at pos %d: %s", pos, input[pos:])
	}

	// Return the tokens
	return &lex{tokens: tokens}, nil
}

type lex struct {
	tokens []token
	filter Filter
}

func (l *lex) Lex(lval *yySymType) int {
	// If we didn't parse any tokens, return 0
	if len(l.tokens) == 0 {
		return 0
	}

	// Pop the first token from the list
	v := l.tokens[0]
	l.tokens = l.tokens[1:]

	// Process the token depending on type
	switch v.typ {
	// If token is a comparator, convert the comparator to it's typed
	// representation and store it in the destination struct
	case tokComp:
		lval.comp = comparatorFromString(v.value)

	// If the token is a tag store it's value in the destination struct
	case tokTag:
		lval.tag = v.value

	// If the token is a value, convert it to an integer and store it in the
	// destination struct
	case tokVal:
		n, err := strconv.Atoi(v.value)
		if err != nil {
			// TODO: Find a way to handle gracefully
			log.Fatal("tagger: invalid integer value")
		}
		lval.val = n
	}

	// Return the type of the tag
	return int(v.typ)
}

func (l *lex) Error(e string) {
	log.Fatal(e)
}
